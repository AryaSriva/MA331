---
name: "Aryaman Srivastava"
pledge: "I pledge my honor that I have abided by the Stevens Honors System"
title: "HW_9"
output: pdf_document
date: "2023-11-20"
---


#HW #9 Due 11:59 PM Monday November 20, 2023

All parts of this homework should be submitted to Canvas. Submit the answers to questions without an inserted code block using handwritten scanned notes or as a latex/typed up pdf file with justification. Submit the answers  to those questions with an inserted code block as the pdf file generated when knitting this report. If need be, combine these pdfs into a single file

##Question 1

How are returns on common stocks in overseas markets related to returns in U.S. markets? Consider measuring U.S. returns by the annual rate of return on the Standard & Poorâ€™s 500 stock index and overseas returns by the annual rate of return on the Morgan Stanley Europe, Australasia, Far East (EAFE)index. Both are recorded in percents. We will regress the EAFE returns on the S&P 500 returns for the years 1993 to 2012. Here is part of the output for this regression:


The regression equation is

EAFE = - 0.168 + 0.845 S&P


Analysis of Variance
Source            DF         SS        MS        F
Regression        1       4947.2     4947.2       26.9
Residual Error    18      3304.3        183.6       0  
Total             19       8251.5     434.3

Fill in all missing values (those with question marks), and draw a conclusion from this table using alpha = .05. Explain what your answer means.

Since the p-value of the data is less than our alpha of 0.05 we reject the null and have sufficient evidence that there is a strong linear relationship between S&P and EAFE. In otherwords, changes in EAFE can be explained by the linear relationship between EAFE and S&P.


##Question 2: From normal to chi-squared to F
###A 
Generate 5000 independent standard normal random variables. Square each of these random variables. Plot the histogram of these results. 
```{r}
vars <- rnorm(5000)
hist(vars^2)
```


Look at the density of a chi-squared distribution with 1 degree of freedom by editing the code below and compare to your histogram 
```{r}
curve(dchisq(x, df = 1),from = 0, to=10 )
```
The shape of both distributions seems to be roughly the same, with a heavy right skew and center of about 0.


###B
For 5000 replicates, generate 20 independent standard normal random variables. Square each of these random variables and take their sum. Plot the histogram of these results. 
```{r}
vars <- replicate(5000, sum(rnorm(20, 0, 1)^2))
hist(vars)
```


Look at the density of a chi-squared distribution with 20 degrees of freedom by editing the code below and compare to your histogram
```{r}
curve(dchisq(x, df = 20),from = 5, to=45 )

```

The shape of both distributions seems to be roughly the same, with an approximately normal shape and center of about 20.


###C 
For 5000 replicates, generate 50 independent standard normal random variables. Square each of these random variables and take their sum. Plot the histogram of these results. 


Look at the density of a chi-squared distribution with 50 degrees of freedom by editing the code below and compare to your histogram
```{r}
vars <- replicate(5000, sum(rnorm(50, 0, 1)^2))
hist(vars)
curve(dchisq(x, df = 50),from = 10, to=90 )


```

The shape of both distributions seems to be roughly the same, with a normal shape and center around 50.


###D
For 5000 replicates, generate a standard normal random variable. Square each of these random variables. Also generate 20 independent standard normal random variables. Square each of these random variables and take their sum. In each replicate, divide the first squared normal RV by 1. Divide  the second sum of squared normal RVs by 20. Divide the first quotient by the second quotient Plot the histogram of these results. 
```{r}
vars <- replicate(5000, (rnorm(1,0,1)^2/1)/(sum(rnorm(20, 0, 1)^2)/20))
hist(vars)
```


Look at the density of an F-distribution with 1 and 20 degrees of freedom by editing the code below and compare to your histogram

```{r}
curve(df(x, df1 = 1, df2 = 20),from = 0, to=10)

```

The shape of both distributions seems to be roughly the same, with a heavy right skew and center of about 0.



For 5000 replicates, generate 20 independent standard normal random variables. Square each of these random variables and take their sum. Also generate 50 independent standard normal random variables. Square each of these random variables and take their sum. In each replicate, divide the first sum of squared normal RVs by 20. Divide the second sum of squared normal RVs by 50. Divide the first quotient by the second quotient. Plot the histogram of these results. 
```{r}
vars <- replicate(5000, (sum(rnorm(20,0,1)^2)/20)/(sum(rnorm(50, 0, 1)^2)/50))
hist(vars)
```



Look at the density of an F-distribution with 20 and 50 degrees of freedom by editing the code below and compare to your histogram

```{r}
curve(df(x, df1 = 20, df2 = 50),from = 0, to=3.3)
```


The shape of both distributions seems to be roughly the same, with a slight right skew and center of about 1.0.


##Question 3: One-Way ANOVA
###A
We fit a one-way ANOVA using the iris dataset, where we look at the different petal widths by species. 
```{r}
aov(Petal.Width~Species, data=iris)

```

Using the output, finish the ANOVA table and draw your conclusion. Use alpha=.05.



Analysis of Variance
Source            DF         SS        MS        F
Regression        2       80.413     40.21       957.38
Residual Error    147     6.156       0.042         
Total             149      86.569    0.581


Since the p-value of the data is less than our alpha of 0.05 we reject the null and have sufficient evidence that there is a strong linear relationship between Petal Witdh and Species. 


###B
Fit a linear model regressing Petal.Width on Species, and print the summary. Using the output of the linear model, and your result from part A, explain how this regression gives us the same information we get from a one-way ANOVA. 
```{r}
model <- lm(Petal.Width~Species, data = iris)
summary(model)
```

This regression gives us the same information we get from a one-way ANOVA as it shows us the F-statistic, which is approximately the same as the one in the table. It also shows us the degrees of freedom for the residuals and regression. Furthermore, it shows us the p-value of the distribution, which we could also use the F-statistic from the table to get. This p-value also leads us to the same conclusion as the one from the table, so whether we used an ANOVA table or a simple linear model, we will get th same result. 